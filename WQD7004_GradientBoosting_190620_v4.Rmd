---
title: "WQD7004_GradientBoosting_190620"
author: "Sofia Atikah"
date: "6/19/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r}
#Define the source file directory, where the file is located.
# Two conditions: 1) dataset with remove outliers and 2) dataset keep outliers
setwd("C:/Users/sofia.zabit/Desktop/WQD7004-Dataset-Ready2use-20200614T134740Z-001")
getwd()
```

```{r}
# Condition 1: Training and testing the model using dataset remove outliers
# Importing the datasets for train, test and out-of-sample
train_RE = read.csv('train_remove_outlier.csv')
test_RE = read.csv('test_remove_outlier.csv')
OOS_RE = read.csv('oos_remove_outlier.csv')

# Fitting XGBoost model to the Train set for model training process
# to install xgboost and import XGboost
# Install.packages('xgboost')
library(xgboost)


# Model 1: Vanilla xgboost with No of Trees = 100
classifier = xgboost(data = as.matrix(train_RE[-1]), label = train_RE$signal, nrounds = 300)

# Predicting the Test set results
y_pred = predict(classifier, newdata = as.matrix(test_RE[-1]))
y_pred = (y_pred >= 0.5)

# Making the Confusion Matrix
cm = table(test_RE[, 1], y_pred)
cm

# Classification Rate
#1) Accuracy:

accuracy = (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1])
accuracy  


###########################################################################################

#Condition 2) Vanilla xgboost with No of Trees = 200
classifier1 = xgboost(data = as.matrix(train_RE[-1]), label = train_RE$signal, nrounds = 200)

# Predicting the Test set results
y_pred1 = predict(classifier1, newdata = as.matrix(test_RE[-1]))
y_pred1 = (y_pred1 >= 0.5)

# Making the Confusion Matrix
cm = table(test_RE[, 1], y_pred1)
cm

accuracy = (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1])
accuracy  

#condition 2.1 Vanilla xgboost with No of Trees = 300
classifier1 = xgboost(data = as.matrix(train_RE[-1]), label = train_RE$signal, nrounds = 300)

# Predicting the Test set results
y_pred1 = predict(classifier1, newdata = as.matrix(test_RE[-1]))
y_pred1 = (y_pred1 >= 0.5)

# Making the Confusion Matrix
cm = table(test_RE[, 1], y_pred1)
cm

accuracy = (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1])
accuracy  
##########################################################################################

# Condition 3) Xgboost parameters with parameter tuning learning rate=0.001

xgb_params <- list(colsample_bytree = 0.7, #how many variables to consider for each tree
                   subsample = 0.75, #how much of the data to use for each tree
                   booster = "gbtree",
                   max_depth = 5, #how many levels in the tree
                   eta = 0.001, #shrinkage rate to control overfitting through conservative approach
                   eval_metric = "Accuracy", 
                   objective = "binary:logistic",
                   gamma = 3)

###########################################################################################

# Conditon 4) Xgboost parameters with parameter tuning learning rate=0.01
xgb_params <- list(colsample_bytree = 0.7, #how many variables to consider for each tree
                   subsample = 0.75, #how much of the data to use for each tree
                   booster = "gbtree",
                   max_depth = 5, #how many levels in the tree
                   eta = 0.01, #shrinkage rate to control overfitting through conservative approach
                   eval_metric = "Accuracy", 
                   objective = "binary:logistic",
                   gamma = 3)


# Conditon 5) Xgboost parameters with parameter tuning learning rate=0.1
xgb_params <- list(colsample_bytree = 0.7, #how many variables to consider for each tree
                   subsample = 0.75, #how much of the data to use for each tree
                   booster = "gbtree",
                   max_depth = 5, #how many levels in the tree
                   eta = 0.1, #shrinkage rate to control overfitting through conservative approach
                   eval_metric = "Accuracy", 
                   objective = "binary:logistic",
                   gamma = 3)

# Conditon 5) Xgboost parameters with parameter tuning learning rate=0.2
xgb_params <- list(colsample_bytree = 0.7, #how many variables to consider for each tree
                   subsample = 0.75, #how much of the data to use for each tree
                   booster = "gbtree",
                   max_depth = 5, #how many levels in the tree
                   eta = 0.2, #shrinkage rate to control overfitting through conservative approach
                   eval_metric = "Accuracy", 
                   objective = "binary:logistic",
                   gamma = 3)

# Conditon 6) Xgboost parameters with parameter tuning learning rate=0.5
xgb_params <- list(colsample_bytree = 0.7, #how many variables to consider for each tree
                   subsample = 0.75, #how much of the data to use for each tree
                   booster = "gbtree",
                   max_depth = 5, #how many levels in the tree
                   eta = 0.5, #shrinkage rate to control overfitting through conservative approach
                   eval_metric = "Accuracy", 
                   objective = "binary:logistic",
                   gamma = 3)
#Training the model

gb_dt = xgboost(xgb_params,data = as.matrix(train_RE[-1]), label = train_RE$signal,nrounds = 100)
gb_dt$params

#Predict using test set

y_pred1 = predict(gb_dt, newdata = as.matrix(test_RE[-1]))
y_pred1 = (y_pred1 >= 0.5)

cm = table(test_RE[, 1], y_pred1)
cm

#Classification Rate
#1) Accuracy:
accuracy = (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1])
accuracy  

################################# predicting the best performing model using OOS accuracy

#Predict using OOS set with remode outliers
# Model 1: Vanilla xgboost with No of Trees = 100
classifier = xgboost(data = as.matrix(train_RE[-1]), label = train_RE$signal, nrounds = 100)

# Predicting the Test set results
y_pred = predict(classifier, newdata = as.matrix(OOS_RE[-1]))
y_pred = (y_pred >= 0.5)

# Making the Confusion Matrix
cm = table(OOS_RE[, 1], y_pred)
cm

# Classification Rate
#1) Accuracy:

accuracy = (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1])
accuracy  


###########################################################################################

#Condition 2) Vanilla xgboost with No of Trees = 200
classifier1 = xgboost(data = as.matrix(train_RE[-1]), label = train_RE$signal, nrounds = 200)

# Predicting the Test set results
y_pred1 = predict(classifier1, newdata = as.matrix(OOS_RE[-1]))
y_pred1 = (y_pred1 >= 0.5)

# Making the Confusion Matrix
cm = table(OOS_RE[, 1], y_pred1)
cm

accuracy = (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1])
accuracy  

#condition 2.1 Vanilla xgboost with No of Trees = 300
classifier1 = xgboost(data = as.matrix(train_RE[-1]), label = train_RE$signal, nrounds = 300)

# Predicting the Test set results
y_pred1 = predict(classifier1, newdata = as.matrix(test_RE[-1]))
y_pred1 = (y_pred1 >= 0.5)

# Making the Confusion Matrix
cm = table(test_RE[, 1], y_pred1)
cm

accuracy = (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1])
accuracy  
##########################################################################################

# Condition 3) Xgboost parameters with parameter tuning learning rate=0.001

xgb_params <- list(colsample_bytree = 0.7, #how many variables to consider for each tree
                   subsample = 0.75, #how much of the data to use for each tree
                   booster = "gbtree",
                   max_depth = 5, #how many levels in the tree
                   eta = 0.001, #shrinkage rate to control overfitting through conservative approach
                   eval_metric = "Accuracy", 
                   objective = "binary:logistic",
                   gamma = 3)

###########################################################################################

# Conditon 4) Xgboost parameters with parameter tuning learning rate=0.01
xgb_params <- list(colsample_bytree = 0.7, #how many variables to consider for each tree
                   subsample = 0.75, #how much of the data to use for each tree
                   booster = "gbtree",
                   max_depth = 5, #how many levels in the tree
                   eta = 0.01, #shrinkage rate to control overfitting through conservative approach
                   eval_metric = "Accuracy", 
                   objective = "binary:logistic",
                   gamma = 3)


# Conditon 5) Xgboost parameters with parameter tuning learning rate=0.1
xgb_params <- list(colsample_bytree = 0.7, #how many variables to consider for each tree
                   subsample = 0.75, #how much of the data to use for each tree
                   booster = "gbtree",
                   max_depth = 5, #how many levels in the tree
                   eta = 0.1, #shrinkage rate to control overfitting through conservative approach
                   eval_metric = "Accuracy", 
                   objective = "binary:logistic",
                   gamma = 3)

# Conditon 5) Xgboost parameters with parameter tuning learning rate=0.2
xgb_params <- list(colsample_bytree = 0.7, #how many variables to consider for each tree
                   subsample = 0.75, #how much of the data to use for each tree
                   booster = "gbtree",
                   max_depth = 5, #how many levels in the tree
                   eta = 0.2, #shrinkage rate to control overfitting through conservative approach
                   eval_metric = "Accuracy", 
                   objective = "binary:logistic",
                   gamma = 3)

# Conditon 6) Xgboost parameters with parameter tuning learning rate=0.5
xgb_params <- list(colsample_bytree = 0.7, #how many variables to consider for each tree
                   subsample = 0.75, #how much of the data to use for each tree
                   booster = "gbtree",
                   max_depth = 5, #how many levels in the tree
                   eta = 0.5, #shrinkage rate to control overfitting through conservative approach
                   eval_metric = "Accuracy", 
                   objective = "binary:logistic",
                   gamma = 3)
#Training the model

gb_dt = xgboost(xgb_params,data = as.matrix(train_RE[-1]), label = train_RE$signal,nrounds = 100)
gb_dt$params

#Predict using test set

y_pred1 = predict(gb_dt, newdata = as.matrix(OOS_RE[-1]))
y_pred1 = (y_pred1 >= 0.5)

cm = table(OOS_RE[, 1], y_pred1)
cm

#Classification Rate
#1) Accuracy:
accuracy = (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1])
accuracy  


##################################################### DATASET KEEP OUTLIERS ############################################################################
###################################################################################################################################################
#################################################################################################################################################

# Importing the dataset that contains outliers

train_KE = read.csv('train_keep_outlier.csv')
test_KE = read.csv('test_keep_outlier.csv')
OOS_KE = read.csv('oos_keep_outlier.csv')


# Fitting XGBoost to the Training set
# install.packages('xgboost')
library(xgboost)

#Condition 1) Vanilla xgboost( without any hyperparameter tuning)
classifier = xgboost(data = as.matrix(train_KE[-1]), label = train_KE$signal, nrounds = 100)

# Predicting the Test set results
y_pred = predict(classifier, newdata = as.matrix(test_KE[-1]))
y_pred = (y_pred >= 0.5)

# Making the Confusion Matrix
cm = table(test_KE[, 1], y_pred)
cm

accuracy = (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1])
accuracy  

#################################################################################################################

#Condition 2) Vanilla xgboost with No of Trees = 200
classifier1 = xgboost(data = as.matrix(train_KE[-1]), label = train_KE$signal, nrounds = 200)

# Predicting the Test set results
y_pred1 = predict(classifier1, newdata = as.matrix(test_KE[-1]))
y_pred1 = (y_pred1 >= 0.5)

# Making the Confusion Matrix
cm = table(test_KE[, 1], y_pred1)
cm

accuracy = (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1])
accuracy  

#Condition 2.1) Vanilla xgboost with No of Trees = 300
classifier1 = xgboost(data = as.matrix(train_KE[-1]), label = train_KE$signal, nrounds = 300)

# Predicting the Test set results
y_pred1 = predict(classifier1, newdata = as.matrix(test_KE[-1]))
y_pred1 = (y_pred1 >= 0.5)

# Making the Confusion Matrix
cm = table(test_KE[, 1], y_pred1)
cm

accuracy = (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1])
accuracy  

####################################################################################################################

#Condition 3) Xgboost with parameter tuning
library(caret)


# Condition 3) Xgboost parameters with parameter tuning learning rate=0.001

xgb_params <- list(colsample_bytree = 0.7, #how many variables to consider for each tree
                   subsample = 0.75, #how much of the data to use for each tree
                   booster = "gbtree",
                   max_depth = 5, #how many levels in the tree
                   eta = 0.001, #shrinkage rate to control overfitting through conservative approach
                   eval_metric = "Accuracy", 
                   objective = "binary:logistic",
                   gamma = 3)

###########################################################################################

# Conditon 4) Xgboost parameters with parameter tuning learning rate=0.01
xgb_params <- list(colsample_bytree = 0.7, #how many variables to consider for each tree
                   subsample = 0.75, #how much of the data to use for each tree
                   booster = "gbtree",
                   max_depth = 5, #how many levels in the tree
                   eta = 0.01, #shrinkage rate to control overfitting through conservative approach
                   eval_metric = "Accuracy", 
                   objective = "binary:logistic",
                   gamma = 3)


# Conditon 5) Xgboost parameters with parameter tuning learning rate=0.1
xgb_params <- list(colsample_bytree = 0.7, #how many variables to consider for each tree
                   subsample = 0.75, #how much of the data to use for each tree
                   booster = "gbtree",
                   max_depth = 5, #how many levels in the tree
                   eta = 0.1, #shrinkage rate to control overfitting through conservative approach
                   eval_metric = "Accuracy", 
                   objective = "binary:logistic",
                   gamma = 3)

# Conditon 5) Xgboost parameters with parameter tuning learning rate=0.2
xgb_params <- list(colsample_bytree = 0.7, #how many variables to consider for each tree
                   subsample = 0.75, #how much of the data to use for each tree
                   booster = "gbtree",
                   max_depth = 5, #how many levels in the tree
                   eta = 0.2, #shrinkage rate to control overfitting through conservative approach
                   eval_metric = "Accuracy", 
                   objective = "binary:logistic",
                   gamma = 3)

# Conditon 6) Xgboost parameters with parameter tuning learning rate=0.5
xgb_params <- list(colsample_bytree = 0.7, #how many variables to consider for each tree
                   subsample = 0.75, #how much of the data to use for each tree
                   booster = "gbtree",
                   max_depth = 5, #how many levels in the tree
                   eta = 0.5, #shrinkage rate to control overfitting through conservative approach
                   eval_metric = "Accuracy", 
                   objective = "binary:logistic",
                   gamma = 3)


#train data
gb_dt = xgboost(xgb_params,data = as.matrix(train_KE[-1]), label = train_KE$signal,nrounds = 100)
gb_dt$params

#Predict using test set
y_pred1 = predict(gb_dt, newdata = as.matrix(test_KE[-1]))
y_pred1 = (y_pred1 >= 0.5)

cm = table(test_KE[, 1], y_pred1)
cm

accuracy1 = (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1])
accuracy1 


##################################### Predict the model accuarcy on the OOS dataset (keep outliers)
#Changing the nrounds to 100,200 and 300. to check on the result of the accuracy
classifier = xgboost(data = as.matrix(train_KE[-1]), label = train_KE$signal, nrounds = 300)

#Predict using test set
y_pred1 = predict(gb_dt, newdata = as.matrix(OOS_KE[-1]))
y_pred1 = (y_pred1 >= 0.5)

cm = table(OOS_KE[, 1], y_pred1)
cm

accuracy1 = (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1])
accuracy1 

#####################################

# Condition 3) Xgboost parameters with parameter tuning learning rate=0.001

xgb_params <- list(colsample_bytree = 0.7, #how many variables to consider for each tree
                   subsample = 0.75, #how much of the data to use for each tree
                   booster = "gbtree",
                   max_depth = 5, #how many levels in the tree
                   eta = 0.001, #shrinkage rate to control overfitting through conservative approach
                   eval_metric = "Accuracy", 
                   objective = "binary:logistic",
                   gamma = 3)

###########################################################################################

# Conditon 4) Xgboost parameters with parameter tuning learning rate=0.01
xgb_params <- list(colsample_bytree = 0.7, #how many variables to consider for each tree
                   subsample = 0.75, #how much of the data to use for each tree
                   booster = "gbtree",
                   max_depth = 5, #how many levels in the tree
                   eta = 0.01, #shrinkage rate to control overfitting through conservative approach
                   eval_metric = "Accuracy", 
                   objective = "binary:logistic",
                   gamma = 3)


# Conditon 5) Xgboost parameters with parameter tuning learning rate=0.1
xgb_params <- list(colsample_bytree = 0.7, #how many variables to consider for each tree
                   subsample = 0.75, #how much of the data to use for each tree
                   booster = "gbtree",
                   max_depth = 5, #how many levels in the tree
                   eta = 0.1, #shrinkage rate to control overfitting through conservative approach
                   eval_metric = "Accuracy", 
                   objective = "binary:logistic",
                   gamma = 3)

# Conditon 5) Xgboost parameters with parameter tuning learning rate=0.2
xgb_params <- list(colsample_bytree = 0.7, #how many variables to consider for each tree
                   subsample = 0.75, #how much of the data to use for each tree
                   booster = "gbtree",
                   max_depth = 5, #how many levels in the tree
                   eta = 0.2, #shrinkage rate to control overfitting through conservative approach
                   eval_metric = "Accuracy", 
                   objective = "binary:logistic",
                   gamma = 3)

# Conditon 6) Xgboost parameters with parameter tuning learning rate=0.5
xgb_params <- list(colsample_bytree = 0.7, #how many variables to consider for each tree
                   subsample = 0.75, #how much of the data to use for each tree
                   booster = "gbtree",
                   max_depth = 5, #how many levels in the tree
                   eta = 0.7, #shrinkage rate to control overfitting through conservative approach
                   eval_metric = "Accuracy", 
                   objective = "binary:logistic",
                   gamma = 3)


#train data
gb_dt = xgboost(xgb_params,data = as.matrix(train_KE[-1]), label = train_KE$signal,nrounds = 100)
gb_dt$params

#Predict using test set
y_pred1 = predict(gb_dt, newdata = as.matrix(OOS_KE[-1]))
y_pred1 = (y_pred1 >= 0.5)

cm = table(OOS_KE[, 1], y_pred1)
cm

accuracy1 = (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1])
accuracy1 



#Result from X-GBoost
#"Parameter_tuning" = c('Vanilla_GBM_n_trees = 100','Vanilla_GBM_n_trees = 200','Vanilla_GBM_n_trees = 300','TuneGBM_n_trees = 100 + ETA = 0.001','TuneGBM_n_trees_100+ETA=0.01','TuneGBM_n_trees_100+ETA=0.1','TuneGBM_n_trees_100+ETA=0.2','TuneGBM_n_trees=100+ETA=0.5'), "Accuracy_RE_Testset" = c(0.717, 0.710, 0.708, 0.7175, 0.7175, 0.7175, 0.7175, 0.7175),"Accuracy_RE_OOSset" = c(0.712, 0.711, 0.708, 0.712, 0.712, 0.712, 0.712, 0.712 ),"Accuracy_KE_Testset" = c(0.713, 0.712, 0.710, 0.714, 0.713, 0.713, 0.713, 0.713),"Accuracy_KE_OOSset" = c(0.719, 0.719, 0.719, 0.719, 0.719, 0.719, 0.719, 0.719)

Result <- data.frame("Param" = c('Vanilla_GBM_n_trees = 100','Vanilla_GBM_n_trees = 200','Vanilla_GBM_n_trees = 300','TuneGBM_n_trees = 100 + ETA = 0.001','TuneGBM_n_trees_100+ETA=0.01','TuneGBM_n_trees_100+ETA=0.1','TuneGBM_n_trees_100+ETA=0.2','TuneGBM_n_trees=100+ETA=0.5'),"Accuracy_RE_Testset" = c(0.717, 0.710, 0.708, 0.7175, 0.7175, 0.7175, 0.7175, 0.7175),"Accuracy_RE_OOSset" = c(0.712, 0.711, 0.708, 0.712, 0.712, 0.712, 0.712, 0.712 ),"Accuracy_KE_Testset" = c(0.713, 0.712, 0.710, 0.714, 0.713, 0.713, 0.713, 0.713),"Accuracy_KE_OOSset" = c(0.719, 0.719, 0.719, 0.719, 0.719, 0.719, 0.719, 0.719)
)
Result
summary(Result)

```
